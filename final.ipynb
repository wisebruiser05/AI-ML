{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e2dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_aug = pd.read_csv('data/bank-full.csv', sep=';')\n",
    "\n",
    "df_train.info()\n",
    "df_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug['y'] = df_aug['y'].map({'yes':1, 'no':0})\n",
    "df_aug.columns = df_train.columns.drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0788553",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ids = df_train['id']\n",
    "df_test_ids = df_test['id']\n",
    "df_train_y = df_train['y']\n",
    "df_aug_y = df_aug['y']\n",
    "\n",
    "\n",
    "\n",
    "#drop unnecessary columns before column trasformation\n",
    "df_train = df_train.drop(columns=['id', 'y'])\n",
    "df_test = df_test.drop(columns=['id'])\n",
    "df_aug = df_aug.drop(columns=['y'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train, df_test, df_aug], ignore_index=True)\n",
    "categorical_cols = df_all.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fb8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('encoder', OneHotEncoder(), categorical_cols),\n",
    "    ],\n",
    "    remainder = 'passthrough'\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_all_transformed = ct.fit_transform(df_all)\n",
    "df_all_transformed = scaler.fit_transform(df_all_transformed)\n",
    "\n",
    "df_all_transformed = pd.DataFrame(df_all_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebbea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_transformed = pd.get_dummies(df_all, columns=categorical_cols, dummy_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_transformed = df_all_transformed.iloc[:len(df_train)]\n",
    "df_test_transformed = df_all_transformed.iloc[len(df_train):len(df_train)+len(df_test)]\n",
    "df_aug_transformed = df_all_transformed.iloc[len(df_train)+len(df_test):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714df46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = pd.concat([df_train_transformed, df_aug_transformed])\n",
    "y_all = pd.concat([df_train_y, df_aug_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8defdd",
   "metadata": {},
   "source": [
    "#### RF + LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb05488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rfClassifier = RandomForestClassifier(n_estimators=300,\n",
    "    max_depth=None,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "lgbmClassifier = lgb.LGBMClassifier(n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,\n",
    "    subsample=0.8,  \n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=[('rf', rfClassifier), ('lgbm', lgbmClassifier)],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    stack_method=\"predict_proba\",\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "stack_model.fit(x_train, y_train)\n",
    "y_pred = stack_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc = roc_auc_score(y_test, y_pred)\n",
    "print(f'ROC AUC: {roc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818999f",
   "metadata": {},
   "source": [
    "#### RF + LightGBM with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a73598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class StackingClassifierOptimizer:\n",
    "    \"\"\"\n",
    "    A comprehensive hyperparameter optimizer for stacking classifiers using Optuna.\n",
    "    This class optimizes Random Forest, LightGBM, and the meta-learner (Logistic Regression).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x_train, y_train, X_val=None, y_val=None, cv_folds=5, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the optimizer with training data and validation strategy.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : array-like\n",
    "            Training features\n",
    "        y_train : array-like  \n",
    "            Training labels\n",
    "        X_val : array-like, optional\n",
    "            Validation features (if None, uses cross-validation)\n",
    "        y_val : array-like, optional\n",
    "            Validation labels (if None, uses cross-validation)\n",
    "        cv_folds : int\n",
    "            Number of cross-validation folds if validation set not provided\n",
    "        random_state : int\n",
    "            Random state for reproducibility\n",
    "        \"\"\"\n",
    "        self.X_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.cv_folds = cv_folds\n",
    "        self.random_state = random_state\n",
    "        self.use_cv = X_val is None or y_val is None\n",
    "        \n",
    "        # Initialize cross-validation if needed\n",
    "        if self.use_cv:\n",
    "            self.cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    def objective(self, trial):\n",
    "        \"\"\"\n",
    "        Objective function for Optuna optimization.\n",
    "        This function defines the hyperparameter search space and returns the metric to optimize.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        trial : optuna.Trial\n",
    "            Optuna trial object for suggesting hyperparameters\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Accuracy score to be maximized\n",
    "        \"\"\"\n",
    "        \n",
    "        # ============================================================================\n",
    "        # STEP 1: Define hyperparameter search spaces for Random Forest\n",
    "        # ============================================================================\n",
    "        rf_params = {\n",
    "            'n_estimators': trial.suggest_int('rf_n_estimators', 100, 500, step=50),\n",
    "            'max_depth': trial.suggest_categorical('rf_max_depth', [None, 10, 20, 30, 50]),\n",
    "            'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 1, 10),\n",
    "            'max_features': trial.suggest_categorical('rf_max_features', ['sqrt', 'log2', None]),\n",
    "            'bootstrap': trial.suggest_categorical('rf_bootstrap', [True, False]),\n",
    "            'random_state': self.random_state,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        # ============================================================================\n",
    "        # STEP 2: Define hyperparameter search spaces for LightGBM\n",
    "        # ============================================================================\n",
    "        lgbm_params = {\n",
    "            'n_estimators': trial.suggest_int('lgbm_n_estimators', 100, 1000, step=50),\n",
    "            'learning_rate': trial.suggest_float('lgbm_learning_rate', 0.01, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('lgbm_max_depth', -1, 50),\n",
    "            'num_leaves': trial.suggest_int('lgbm_num_leaves', 10, 300),\n",
    "            'subsample': trial.suggest_float('lgbm_subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('lgbm_colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('lgbm_reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('lgbm_reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'min_child_samples': trial.suggest_int('lgbm_min_child_samples', 5, 100),\n",
    "            'random_state': self.random_state,\n",
    "            'n_jobs': -1,\n",
    "            'verbose': -1  # Suppress LightGBM output\n",
    "        }\n",
    "        \n",
    "        # ============================================================================\n",
    "        # STEP 3: Define hyperparameter search spaces for Logistic Regression (Meta-learner)\n",
    "        # ============================================================================\n",
    "        lr_params = {\n",
    "            'C': trial.suggest_float('lr_C', 1e-5, 100, log=True),\n",
    "            'solver': trial.suggest_categorical('lr_solver', ['liblinear', 'lbfgs']),\n",
    "            'max_iter': trial.suggest_int('lr_max_iter', 100, 1000),\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "        \n",
    "        # Handle penalty parameter based on solver\n",
    "        if lr_params['solver'] == 'liblinear':\n",
    "            lr_params['penalty'] = trial.suggest_categorical('lr_penalty', ['l1', 'l2'])\n",
    "        else:  # lbfgs\n",
    "            lr_params['penalty'] = 'l2'  # lbfgs only supports l2\n",
    "        \n",
    "        # ============================================================================\n",
    "        # STEP 4: Create base estimators with suggested hyperparameters\n",
    "        # ============================================================================\n",
    "        rf_classifier = RandomForestClassifier(**rf_params)\n",
    "        lgbm_classifier = lgb.LGBMClassifier(**lgbm_params)\n",
    "        \n",
    "        # ============================================================================\n",
    "        # STEP 5: Create the stacking classifier\n",
    "        # ============================================================================\n",
    "        stack_model = StackingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', rf_classifier), \n",
    "                ('lgbm', lgbm_classifier)\n",
    "            ],\n",
    "            final_estimator=LogisticRegression(**lr_params),\n",
    "            cv=5,  # Internal cross-validation for generating meta-features\n",
    "            stack_method='predict_proba',\n",
    "            n_jobs=-1,\n",
    "            passthrough=False\n",
    "        )\n",
    "        \n",
    "        # ============================================================================\n",
    "        # STEP 6: Evaluate the model using cross-validation or validation set\n",
    "        # ============================================================================\n",
    "        try:\n",
    "            if self.use_cv:\n",
    "                # Use cross-validation for evaluation\n",
    "                scores = cross_val_score(\n",
    "                    stack_model, self.X_train, self.y_train, \n",
    "                    cv=self.cv, scoring='accuracy', n_jobs=-1\n",
    "                )\n",
    "                accuracy = scores.mean()\n",
    "            else:\n",
    "                # Use separate validation set\n",
    "                stack_model.fit(self.X_train, self.y_train)\n",
    "                y_pred = stack_model.predict(self.X_val)\n",
    "                accuracy = accuracy_score(self.y_val, y_pred)\n",
    "            \n",
    "            return accuracy\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Return a low score if the model fails to train\n",
    "            print(f\"Trial failed with error: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def optimize(self, n_trials=100, timeout=None, show_progress=True):\n",
    "        \"\"\"\n",
    "        Run the hyperparameter optimization process.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_trials : int\n",
    "            Number of optimization trials\n",
    "        timeout : int, optional\n",
    "            Maximum time in seconds for optimization\n",
    "        show_progress : bool\n",
    "            Whether to show optimization progress\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (study object, best parameters, best score)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"STARTING OPTUNA HYPERPARAMETER OPTIMIZATION\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Optimization Strategy: {'Cross-Validation' if self.use_cv else 'Validation Set'}\")\n",
    "        print(f\"Number of trials: {n_trials}\")\n",
    "        print(f\"Timeout: {timeout if timeout else 'None'}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # ============================================================================\n",
    "        # STEP 7: Create and configure the Optuna study\n",
    "        # ============================================================================\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',  # We want to maximize accuracy\n",
    "            pruner=optuna.pruners.MedianPruner(  # Prune unpromising trials early\n",
    "                n_startup_trials=10,\n",
    "                n_warmup_steps=5\n",
    "            ),\n",
    "            sampler=optuna.samplers.TPESampler(seed=self.random_state)  # Tree-structured Parzen Estimator\n",
    "        )\n",
    "        \n",
    "        # ============================================================================\n",
    "        # STEP 8: Run the optimization\n",
    "        # ============================================================================\n",
    "        if show_progress:\n",
    "            # Add callback to show progress\n",
    "            def callback(study, trial):\n",
    "                if trial.number % 10 == 0:\n",
    "                    print(f\"Trial {trial.number}: Best score so far = {study.best_value:.4f}\")\n",
    "        else:\n",
    "            callback = None\n",
    "            \n",
    "        study.optimize(\n",
    "            self.objective,\n",
    "            n_trials=n_trials,\n",
    "            timeout=timeout,\n",
    "            callbacks=[callback] if callback else None,\n",
    "            show_progress_bar=show_progress\n",
    "        )\n",
    "        \n",
    "        # ============================================================================\n",
    "        # STEP 9: Extract and organize best parameters\n",
    "        # ============================================================================\n",
    "        best_params = study.best_params\n",
    "        \n",
    "        # Separate parameters for each model\n",
    "        rf_best_params = {k.replace('rf_', ''): v for k, v in best_params.items() if k.startswith('rf_')}\n",
    "        lgbm_best_params = {k.replace('lgbm_', ''): v for k, v in best_params.items() if k.startswith('lgbm_')}\n",
    "        lr_best_params = {k.replace('lr_', ''): v for k, v in best_params.items() if k.startswith('lr_')}\n",
    "        \n",
    "        # Add fixed parameters\n",
    "        rf_best_params.update({'random_state': self.random_state, 'n_jobs': -1})\n",
    "        lgbm_best_params.update({'random_state': self.random_state, 'n_jobs': -1, 'verbose': -1})\n",
    "        lr_best_params.update({'random_state': self.random_state})\n",
    "        \n",
    "        organized_params = {\n",
    "            'random_forest': rf_best_params,\n",
    "            'lightgbm': lgbm_best_params,\n",
    "            'logistic_regression': lr_best_params,\n",
    "            'best_score': study.best_value\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"OPTIMIZATION COMPLETED!\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Best accuracy: {study.best_value:.4f}\")\n",
    "        print(f\"Number of completed trials: {len(study.trials)}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return study, organized_params, study.best_value\n",
    "    \n",
    "    def create_optimized_model(self, best_params):\n",
    "        \"\"\"\n",
    "        Create the optimized stacking classifier using best parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        best_params : dict\n",
    "            Dictionary containing best parameters for each model\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        StackingClassifier\n",
    "            Optimized stacking classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create optimized base estimators\n",
    "        rf_optimized = RandomForestClassifier(**best_params['random_forest'])\n",
    "        lgbm_optimized = lgb.LGBMClassifier(**best_params['lightgbm'])\n",
    "        lr_optimized = LogisticRegression(**best_params['logistic_regression'])\n",
    "        \n",
    "        # Create optimized stacking classifier\n",
    "        optimized_stack = StackingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', rf_optimized),\n",
    "                ('lgbm', lgbm_optimized)\n",
    "            ],\n",
    "            final_estimator=lr_optimized,\n",
    "            cv=5,\n",
    "            stack_method='predict_proba',\n",
    "            n_jobs=-1,\n",
    "            passthrough=False\n",
    "        )\n",
    "        \n",
    "        return optimized_stack\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "def run_optimization_example(x_train, y_train, X_val=None, y_val=None):\n",
    "    \"\"\"\n",
    "    Example function showing how to use the optimizer.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, y_train : Training data\n",
    "    X_val, y_val : Optional validation data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the optimizer\n",
    "    optimizer = StackingClassifierOptimizer(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        cv_folds=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    study, best_params, best_score = optimizer.optimize(\n",
    "        n_trials=50,  # Adjust based on computational budget\n",
    "        timeout=3600,  # 1 hour timeout\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    # Create the optimized model\n",
    "    optimized_model = optimizer.create_optimized_model(best_params)\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nBEST PARAMETERS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Random Forest:\")\n",
    "    for param, value in best_params['random_forest'].items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(\"\\nLightGBM:\")\n",
    "    for param, value in best_params['lightgbm'].items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(\"\\nLogistic Regression (Meta-learner):\")\n",
    "    for param, value in best_params['logistic_regression'].items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nBest Cross-Validation Accuracy: {best_score:.4f}\")\n",
    "    \n",
    "    # Train the final model\n",
    "    print(\"\\nTraining optimized model on full training set...\")\n",
    "    optimized_model.fit(X_train, y_train)\n",
    "    \n",
    "    return optimized_model, study, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model, study, best_params = run_optimization_example(\n",
    "    x_train, y_train\n",
    ")\n",
    "\n",
    "predictions = optimized_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd2cff1",
   "metadata": {},
   "source": [
    "#### RF + GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214476a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class TQDMStack(StackingClassifier):\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        self.estimators_ = []\n",
    "        # loop through base estimators with tqdm\n",
    "        for name, estimator in tqdm(self.estimators, desc=\"Fitting base estimators\"):\n",
    "            fitted_est = estimator.fit(X, y, **fit_params)\n",
    "            self.estimators_.append(fitted_est)\n",
    "        \n",
    "        # convert to array for stacking logic\n",
    "        self.estimators_ = np.array(self.estimators_, dtype=object)\n",
    "        \n",
    "        # now let the parent class handle the meta learner fit\n",
    "        return super().fit(X, y, **fit_params)\n",
    "\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=200)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=200))\n",
    "]\n",
    "stack = TQDMStack(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    stack_method=\"predict_proba\"\n",
    ")\n",
    "\n",
    "stack.fit(x_train, y_train)\n",
    "stack_y_pred_prob = stack.predict_proba(x_test)[:, 1]\n",
    "stack_roc_auc = roc_auc_score(y_test, stack_y_pred_prob)\n",
    "print(f\"Stacking Classifier ROC AUC: {stack_roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f1e89",
   "metadata": {},
   "source": [
    "#### Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf71aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = stack_model.predict_proba(df_test_transformed)[:, 1]\n",
    "submission = pd.DataFrame({\n",
    "    'id' : df_test_ids,\n",
    "    'y' : final\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"submission.csv file has been created.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a6f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
