{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- 0. Data Preparation ---\n",
    "print(\"--- 0. Starting Data Preparation ---\")\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "augmentation_df = pd.read_csv('data/bank-full.csv', sep=';')\n",
    "\n",
    "# --- FIX: Map target variable in augmentation data ---\n",
    "# Map 'no' to 0 and 'yes' to 1 to match the main dataset's format\n",
    "print(\"Mapping target variable 'y' in the augmentation dataset ('no'->0, 'yes'->1)...\")\n",
    "augmentation_df['y'] = augmentation_df['y'].map({'no': 0, 'yes': 1})\n",
    "\n",
    "# Rename augmentation data columns to match the main dataset\n",
    "print(\"Renaming columns in the augmentation dataset...\")\n",
    "# The target column 'y' already matches.\n",
    "augmentation_df.columns = train_df.columns.drop('id')\n",
    "\n",
    "print(f\"Main training data rows: {len(train_df)}\")\n",
    "print(f\"Augmentation data rows: {len(augmentation_df)}\")\n",
    "print(f\"Test data rows: {len(test_df)}\")\n",
    "print(\"--- Data Preparation Complete ---\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 1. Feature Engineering and Encoding ---\n",
    "print(\"--- 1. Starting Feature Engineering & Encoding ---\")\n",
    "\n",
    "# Store IDs and target variable\n",
    "train_ids = train_df['id']\n",
    "test_ids = test_df['id']\n",
    "y_main = train_df['y']\n",
    "y_aug = augmentation_df['y']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_df = train_df.drop(columns=['id', 'y'])\n",
    "test_df = test_df.drop(columns=['id'])\n",
    "augmentation_df = augmentation_df.drop(columns=['y'])\n",
    "\n",
    "# Combine all dataframes for consistent encoding\n",
    "all_data = pd.concat([train_df, test_df, augmentation_df], ignore_index=True)\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_features = all_data.select_dtypes(include=['object']).columns\n",
    "print(f\"Applying one-hot encoding to: {list(categorical_features)}\")\n",
    "\n",
    "# Apply one-hot encoding\n",
    "all_data_encoded = pd.get_dummies(all_data, columns=categorical_features, dummy_na=False)\n",
    "\n",
    "# Separate back into their original sets\n",
    "X_main = all_data_encoded.iloc[:len(train_df)]\n",
    "X_test = all_data_encoded.iloc[len(train_df):len(train_df) + len(test_df)]\n",
    "X_aug = all_data_encoded.iloc[len(train_df) + len(test_df):]\n",
    "\n",
    "print(f\"Main training features shape: {X_main.shape}\")\n",
    "print(f\"Augmentation features shape: {X_aug.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(\"--- Feature Engineering Complete ---\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 2. Comparative Cross-Validation ---\n",
    "print(\"--- 2. Starting Comparative Cross-Validation ---\")\n",
    "\n",
    "N_SPLITS = 10\n",
    "MAX_AUG_REPEATS = 4\n",
    "results = []\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_estimators': 2000,\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 20,\n",
    "    'max_depth': 5,\n",
    "    'seed': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'subsample': 0.7,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "}\n",
    "\n",
    "# Loop to test different numbers of augmentation repeats\n",
    "for n_repeats in range(MAX_AUG_REPEATS + 1):\n",
    "    print(f\"\\n===== Running CV with augmentation data repeated {n_repeats} time(s) =====\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "    \n",
    "    oof_preds = np.zeros(len(X_main))\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "\n",
    "    # The CV split is ALWAYS based on the main training data\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_main, y_main)):\n",
    "        \n",
    "        # --- Data setup for the fold ---\n",
    "        X_train_fold, y_train_fold = X_main.iloc[train_idx], y_main.iloc[train_idx]\n",
    "        X_val_fold, y_val_fold = X_main.iloc[val_idx], y_main.iloc[val_idx]\n",
    "\n",
    "        # Augment the training data for this fold\n",
    "        if n_repeats > 0:\n",
    "            X_train_augmented = pd.concat([X_train_fold] + [X_aug] * n_repeats, ignore_index=True)\n",
    "            y_train_augmented = pd.concat([y_train_fold] + [y_aug] * n_repeats, ignore_index=True)\n",
    "        else:\n",
    "            X_train_augmented, y_train_augmented = X_train_fold, y_train_fold\n",
    "\n",
    "        # --- Model training ---\n",
    "        model = lgb.LGBMClassifier(**lgb_params)\n",
    "        model.fit(X_train_augmented, y_train_augmented,\n",
    "                  eval_set=[(X_val_fold, y_val_fold)],\n",
    "                  callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "        # --- Prediction ---\n",
    "        val_preds = model.predict_proba(X_val_fold)[:, 1]\n",
    "        oof_preds[val_idx] = val_preds\n",
    "        test_preds += model.predict_proba(X_test)[:, 1] / N_SPLITS\n",
    "\n",
    "    # --- Store results for this n_repeats run ---\n",
    "    overall_oof_auc = roc_auc_score(y_main, oof_preds)\n",
    "    print(f\"Overall OOF AUC for {n_repeats} repeats: {overall_oof_auc:.6f}\")\n",
    "    results.append({\n",
    "        'repeats': n_repeats,\n",
    "        'oof_auc': overall_oof_auc,\n",
    "        'test_predictions': test_preds\n",
    "    })\n",
    "\n",
    "print(\"\\n--- Cross-Validation Comparison Complete ---\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 3. Final Results Summary and Submission ---\n",
    "print(\"--- 3. Final Results and Submission ---\")\n",
    "\n",
    "# Find the best result based on OOF AUC\n",
    "best_result = max(results, key=lambda x: x['oof_auc'])\n",
    "\n",
    "print(\"Summary of OOF AUC by number of augmentation repeats:\")\n",
    "for res in results:\n",
    "    highlight = \"<-- BEST\" if res['repeats'] == best_result['repeats'] else \"\"\n",
    "    print(f\"  Repeats: {res['repeats']}, OOF AUC: {res['oof_auc']:.6f} {highlight}\")\n",
    "\n",
    "print(f\"\\nSelecting predictions from the best run ({best_result['repeats']} repeats).\")\n",
    "\n",
    "# Create submission file using the best predictions\n",
    "submission_df = pd.DataFrame({'id': test_ids, 'y': best_result['test_predictions']})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'submission.csv' has been saved successfully.\")\n",
    "print(\"--- All processes completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f741510",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
